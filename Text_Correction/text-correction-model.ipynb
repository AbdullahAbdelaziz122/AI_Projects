{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"3c829166fae844d3b4a2faa5af06fa02":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_64a86d1bdbf040478179272a46074b38","IPY_MODEL_e4c1d7b3835044b7ba95c08f73303a6d","IPY_MODEL_bbf4653f693d479da78b97c212c11eca"],"layout":"IPY_MODEL_98999775e58f4cfa8fcef3c50a16a5fb"}},"64a86d1bdbf040478179272a46074b38":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8e0c2f8fdb4f40019dd95614f76e506f","placeholder":"​","style":"IPY_MODEL_9721b6acd168493eb2f59e7b294ed93e","value":"Map: 100%"}},"e4c1d7b3835044b7ba95c08f73303a6d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_244a88ff8640431daf3966bb2b5a3f82","max":2988,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b1b7e41822ac403e850479b432ca874c","value":2988}},"bbf4653f693d479da78b97c212c11eca":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_99f2f870a28643b5aa84e78970fb0130","placeholder":"​","style":"IPY_MODEL_fc6e7025e382433f96db66c04fd40493","value":" 2988/2988 [00:02&lt;00:00, 1211.35 examples/s]"}},"98999775e58f4cfa8fcef3c50a16a5fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e0c2f8fdb4f40019dd95614f76e506f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9721b6acd168493eb2f59e7b294ed93e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"244a88ff8640431daf3966bb2b5a3f82":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1b7e41822ac403e850479b432ca874c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"99f2f870a28643b5aa84e78970fb0130":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc6e7025e382433f96db66c04fd40493":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"311aaa3443034141b1fbcaa0c73ac962":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fda9b2ddbdbd4b54b83f30ff9553061f","IPY_MODEL_80f4bd02cbec41cb99d2a07292683363","IPY_MODEL_27e723d3737544ae9294ac0cf71b33e2"],"layout":"IPY_MODEL_bb60eef2ba5446fb95a93b8f71103571"}},"fda9b2ddbdbd4b54b83f30ff9553061f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_960ded2d258d4d7cbf941295a18768e3","placeholder":"​","style":"IPY_MODEL_5bf2b034e66449cd99dc188d827c073b","value":"Map: 100%"}},"80f4bd02cbec41cb99d2a07292683363":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_458ad0865b4c4b81a25866c925f4de0e","max":3016,"min":0,"orientation":"horizontal","style":"IPY_MODEL_16b0ced5c74e48fea3be5ed8c5df05f6","value":3016}},"27e723d3737544ae9294ac0cf71b33e2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6673f8d495ba4e5cbe0bc74d8da22a21","placeholder":"​","style":"IPY_MODEL_e2d1c0aaab0d4d2585fe88ccf46242a8","value":" 3016/3016 [00:01&lt;00:00, 2620.15 examples/s]"}},"bb60eef2ba5446fb95a93b8f71103571":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"960ded2d258d4d7cbf941295a18768e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5bf2b034e66449cd99dc188d827c073b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"458ad0865b4c4b81a25866c925f4de0e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"16b0ced5c74e48fea3be5ed8c5df05f6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6673f8d495ba4e5cbe0bc74d8da22a21":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2d1c0aaab0d4d2585fe88ccf46242a8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-Tune a Transformer Model for Grammar Correction","metadata":{"id":"uK1EI9qgohoq"}},{"cell_type":"markdown","source":"# Installation","metadata":{"id":"yRoJX6u_rPHK"}},{"cell_type":"markdown","source":"First I will try to fine tune the \"grammer_correction\" model on Hugging face\nlink: https://huggingface.co/HamadML/grammer_correction\n\nTraining dataset : https://huggingface.co/datasets/jhu-clsp/jfleg?ref=vennify.ai","metadata":{"id":"3UjalNntonUj"}},{"cell_type":"code","source":"! pip install datasets","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t7L_0i8TqODt","outputId":"8c9d52ca-ea89-43cd-eb2c-ab81326f9330","execution":{"iopub.status.busy":"2024-07-28T11:15:59.253863Z","iopub.execute_input":"2024-07-28T11:15:59.254232Z","iopub.status.idle":"2024-07-28T11:16:13.415839Z","shell.execute_reply.started":"2024-07-28T11:15:59.254201Z","shell.execute_reply":"2024-07-28T11:16:13.414639Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.20.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.23.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.7.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Model","metadata":{"id":"CTPvG0EmrVB-"}},{"cell_type":"code","source":"# Loading Model from Hugging face\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"HamadML/grammer_correction\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"HamadML/grammer_correction\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ktN1ejhwlu9i","outputId":"a732b37f-e166-4507-a99e-ccc930870017","execution":{"iopub.status.busy":"2024-07-28T11:16:40.330559Z","iopub.execute_input":"2024-07-28T11:16:40.330981Z","iopub.status.idle":"2024-07-28T11:16:41.591496Z","shell.execute_reply.started":"2024-07-28T11:16:40.330954Z","shell.execute_reply":"2024-07-28T11:16:41.590696Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Collection","metadata":{"id":"OGLjxZKHrXPe"}},{"cell_type":"code","source":"# Loading Dataset from hugging face\nfrom datasets import load_dataset\n\nds = load_dataset(\"jhu-clsp/jfleg\")","metadata":{"id":"-h90QUXepoWl","execution":{"iopub.status.busy":"2024-07-28T11:16:41.592770Z","iopub.execute_input":"2024-07-28T11:16:41.593127Z","iopub.status.idle":"2024-07-28T11:16:44.717823Z","shell.execute_reply.started":"2024-07-28T11:16:41.593098Z","shell.execute_reply":"2024-07-28T11:16:44.716813Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/5.94k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0738efa3832f412fa6234e9b28fae8f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/148k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a952efa467546ed90ec3faa2e70929a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/141k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c20b44720cb43e3bf68d016ed33ad4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/755 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f49d6a30e3b141a6b2bf869b553259ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/748 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"037da6d3122d4fc988993d97a1df0fd3"}},"metadata":{}}]},{"cell_type":"markdown","source":"","metadata":{"id":"kbD7SCFIobI3"}},{"cell_type":"code","source":"ds.shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-jjOP2rMppe-","outputId":"08d43186-5d41-475d-f3d7-f5bbca8af9d6","execution":{"iopub.status.busy":"2024-07-28T11:17:03.368761Z","iopub.execute_input":"2024-07-28T11:17:03.369733Z","iopub.status.idle":"2024-07-28T11:17:03.376312Z","shell.execute_reply.started":"2024-07-28T11:17:03.369700Z","shell.execute_reply":"2024-07-28T11:17:03.375408Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"{'validation': (755, 2), 'test': (748, 2)}"},"metadata":{}}]},{"cell_type":"code","source":"train_ds = ds['validation']\nval_ds = ds['test']","metadata":{"id":"SaHaTETNppL1","execution":{"iopub.status.busy":"2024-07-28T11:17:05.116309Z","iopub.execute_input":"2024-07-28T11:17:05.117138Z","iopub.status.idle":"2024-07-28T11:17:05.121746Z","shell.execute_reply.started":"2024-07-28T11:17:05.117098Z","shell.execute_reply":"2024-07-28T11:17:05.120495Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Data Examination","metadata":{"id":"ND6PTmyDrJB3"}},{"cell_type":"code","source":"train_ds.column_names, val_ds.column_names","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6mA35hS6q8Lq","outputId":"0db5066e-5c43-4031-8eac-4ac15d785a9e","execution":{"iopub.status.busy":"2024-07-28T11:17:07.385371Z","iopub.execute_input":"2024-07-28T11:17:07.385811Z","iopub.status.idle":"2024-07-28T11:17:07.392557Z","shell.execute_reply.started":"2024-07-28T11:17:07.385775Z","shell.execute_reply":"2024-07-28T11:17:07.391590Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(['sentence', 'corrections'], ['sentence', 'corrections'])"},"metadata":{}}]},{"cell_type":"code","source":"print(train_ds[\"sentence\"][0])\nprint(\"Corrections : \")\nfor i in train_ds[\"corrections\"][0]:\n    print(f\" - {i}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kf76DDPrro5Y","outputId":"d0ff3cf4-4a72-4408-8097-82fe70ac4f93","execution":{"iopub.status.busy":"2024-07-28T11:17:09.339098Z","iopub.execute_input":"2024-07-28T11:17:09.339932Z","iopub.status.idle":"2024-07-28T11:17:09.359368Z","shell.execute_reply.started":"2024-07-28T11:17:09.339896Z","shell.execute_reply":"2024-07-28T11:17:09.358328Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"So I think we can not live if old people could not find siences and tecnologies and they did not developped . \nCorrections : \n - So I think we would not be alive if our ancestors did not develop sciences and technologies . \n - So I think we could not live if older people did not develop science and technologies . \n - So I think we can not live if old people could not find science and technologies and they did not develop . \n - So I think we can not live if old people can not find the science and technology that has not been developed . \n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Data PreProcessing","metadata":{"id":"8P2APT5IsskQ"}},{"cell_type":"markdown","source":"We need to structure both of the training and evaluating data into the same format, which is a CSV file with two columns: input and target. The input column contains grammatically incorrect text, and the target column contains text that is the corrected version of the text from the target column.","metadata":{"id":"00duJre4sxSN"}},{"cell_type":"markdown","source":"Below is code that processes data into the proper format. We must specify the task we wish to perform by adding the same prefix to each input. In this case, **we'll use the prefix \"grammar: \".** This is done because T5 models are able to perform multiple tasks like translation and summarization with a single model, and a unique prefix is used for each task so that the model learns which task to perform. We also need to skip over cases that contain a blank string to avoid errors while fine-tuning.","metadata":{"id":"6nB2unLcs9cl"}},{"cell_type":"code","source":"import csv\n\ndef generate_csv(csv_path, dataset):\n    with open(csv_path, 'w', newline='') as csvfile:\n        writter = csv.writer(csvfile)\n        writter.writerow([\"input\", \"target\"])\n        for case in dataset:\n     \t    # Adding the task's prefix to input\n            input_text = \"grammar: \" + case[\"sentence\"]\n            for correction in case[\"corrections\"]:\n                # a few of the cases contain blank strings.\n                if input_text and correction:\n                    writter.writerow([input_text, correction])\n\n\n\ngenerate_csv(\"train.csv\", train_ds)\ngenerate_csv(\"eval.csv\", val_ds)","metadata":{"id":"jO-KZrOEso4g","execution":{"iopub.status.busy":"2024-07-28T11:17:12.163287Z","iopub.execute_input":"2024-07-28T11:17:12.164027Z","iopub.status.idle":"2024-07-28T11:17:12.299486Z","shell.execute_reply.started":"2024-07-28T11:17:12.163991Z","shell.execute_reply":"2024-07-28T11:17:12.298595Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Before Training Evaluation\nEvaluate the model before fine-tuning to check if the loss decreased after training this means that the model learnded.","metadata":{"id":"0SGBCkUjt5Ol"}},{"cell_type":"markdown","source":"## Load Data","metadata":{"id":"_9qNUEa8w3Or"}},{"cell_type":"code","source":"import pandas as pd\n# Loading data\neval_data = pd.read_csv(\"eval.csv\")\neval_data[\"original\"] = eval_data[\"input\"]\neval_data[\"corrected\"] = eval_data[\"target\"]\neval_data = eval_data.drop(columns=[\"input\", \"target\"])\neval_data.head()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"gAFQoz-qufUD","outputId":"9a98f522-9278-42db-b520-63f5e8e6ddd5","execution":{"iopub.status.busy":"2024-07-28T11:17:31.439468Z","iopub.execute_input":"2024-07-28T11:17:31.439844Z","iopub.status.idle":"2024-07-28T11:17:31.481710Z","shell.execute_reply.started":"2024-07-28T11:17:31.439814Z","shell.execute_reply":"2024-07-28T11:17:31.480637Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"                                            original  \\\n0  grammar: New and new technology has been intro...   \n1  grammar: New and new technology has been intro...   \n2  grammar: New and new technology has been intro...   \n3  grammar: New and new technology has been intro...   \n4  grammar: One possible outcome is that an envir...   \n\n                                           corrected  \n0    New technology has been introduced to society .  \n1  New technology has been introduced into the so...  \n2  Newer and newer technology has been introduced...  \n3  Newer and newer technology has been introduced...  \n4  One possible outcome is that an environmentall...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>original</th>\n      <th>corrected</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>grammar: New and new technology has been intro...</td>\n      <td>New technology has been introduced to society .</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>grammar: New and new technology has been intro...</td>\n      <td>New technology has been introduced into the so...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>grammar: New and new technology has been intro...</td>\n      <td>Newer and newer technology has been introduced...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>grammar: New and new technology has been intro...</td>\n      <td>Newer and newer technology has been introduced...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>grammar: One possible outcome is that an envir...</td>\n      <td>One possible outcome is that an environmentall...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Tokenize input Sentences","metadata":{"id":"jw7uP4IBw6NP"}},{"cell_type":"code","source":"","metadata":{"id":"Q7e1F6IW0NA6"},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\nimport pandas as pd\neval_dataset = eval_data[['original', 'corrected']]\neval_dataset = Dataset.from_pandas(eval_data)\n\n# Tokenize the dataset\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\ndef tokenize_function(examples):\n    inputs = tokenizer(examples[\"original\"], max_length=128, truncation=True, padding=\"max_length\")\n    targets = tokenizer(examples[\"corrected\"], max_length=128, truncation=True, padding=\"max_length\")\n    inputs[\"labels\"] = targets[\"input_ids\"]\n    return inputs\n\neval_dataset = eval_dataset.map(tokenize_function, batched=True)\neval_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n\n# Create data collator\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["3c829166fae844d3b4a2faa5af06fa02","64a86d1bdbf040478179272a46074b38","e4c1d7b3835044b7ba95c08f73303a6d","bbf4653f693d479da78b97c212c11eca","98999775e58f4cfa8fcef3c50a16a5fb","8e0c2f8fdb4f40019dd95614f76e506f","9721b6acd168493eb2f59e7b294ed93e","244a88ff8640431daf3966bb2b5a3f82","b1b7e41822ac403e850479b432ca874c","99f2f870a28643b5aa84e78970fb0130","fc6e7025e382433f96db66c04fd40493"]},"id":"-moJ9fRSxrdp","outputId":"de702ae6-d1e5-40af-8272-d0016cc53c9b","execution":{"iopub.status.busy":"2024-07-28T11:17:39.696249Z","iopub.execute_input":"2024-07-28T11:17:39.696636Z","iopub.status.idle":"2024-07-28T11:17:52.162323Z","shell.execute_reply.started":"2024-07-28T11:17:39.696607Z","shell.execute_reply":"2024-07-28T11:17:52.161280Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"2024-07-28 11:17:41.982281: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-28 11:17:41.982385: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-28 11:17:42.123448: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2988 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8f047f4f43e44079a9b17f0c187a7b4"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\n\n# Set up training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    per_device_eval_batch_size=8,    # Adjust based on available RAM\n    fp16=True  # Enable mixed precision training\n)\n\n# Set up the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    eval_dataset=eval_dataset\n)\n\n# Evaluate the model\nresults = trainer.evaluate()\nprint(results)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":109},"id":"6YMUHNgX1EJg","outputId":"0f28014b-a3e7-48b4-f056-b4ef69f1b116"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:656: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n"},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='374' max='374' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [374/374 00:24]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":"{'eval_loss': 16.770992279052734, 'eval_runtime': 26.2515, 'eval_samples_per_second': 113.822, 'eval_steps_per_second': 14.247}\n"}]},{"cell_type":"markdown","source":"## GLEU Score","metadata":{"id":"du4UpJ7E4ZZA"}},{"cell_type":"code","source":"from nltk.translate.gleu_score import sentence_gleu\nfrom transformers import Trainer, TrainingArguments\n\nimport torch\n\ntorch.cuda.empty_cache()\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    gleu_scores = []\n    for pred, label in zip(decoded_preds, decoded_labels):\n        gleu_scores.append(sentence_gleu([label.split()], pred.split()))\n\n    average_gleu_score = sum(gleu_scores) / len(gleu_scores)\n    return {\"gleu\": average_gleu_score}\n\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    per_device_eval_batch_size=4,  # Reduced batch size\n    fp16=True  # Enable mixed precision training\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    eval_dataset=eval_dataset,\n    compute_metrics=compute_metrics\n)\n\n# Evaluate the model\nresults = trainer.evaluate()\nprint(results)\n\n","metadata":{"id":"V8JUKvEz1b6r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{"id":"Nh0SW4817RWc"}},{"cell_type":"code","source":"train_data = pd.read_csv(\"train.csv\")\ntrain_data[\"original\"] = train_data[\"input\"]\ntrain_data[\"corrected\"] = train_data[\"target\"]\ntrain_data = train_data.drop(columns=[\"input\", \"target\"])\n\ntrain_dataset = [['original', 'corrected']]\ntrain_dataset = Dataset.from_pandas(train_data)\n\n# Tokenize the training data\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\ntrain_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["311aaa3443034141b1fbcaa0c73ac962","fda9b2ddbdbd4b54b83f30ff9553061f","80f4bd02cbec41cb99d2a07292683363","27e723d3737544ae9294ac0cf71b33e2","bb60eef2ba5446fb95a93b8f71103571","960ded2d258d4d7cbf941295a18768e3","5bf2b034e66449cd99dc188d827c073b","458ad0865b4c4b81a25866c925f4de0e","16b0ced5c74e48fea3be5ed8c5df05f6","6673f8d495ba4e5cbe0bc74d8da22a21","e2d1c0aaab0d4d2585fe88ccf46242a8"]},"id":"kOBNmiAV7rsj","outputId":"4fb901ff-06e8-4325-8cc7-192b4b37a4aa","execution":{"iopub.status.busy":"2024-07-28T11:18:01.599580Z","iopub.execute_input":"2024-07-28T11:18:01.599991Z","iopub.status.idle":"2024-07-28T11:18:02.556051Z","shell.execute_reply.started":"2024-07-28T11:18:01.599960Z","shell.execute_reply":"2024-07-28T11:18:02.554906Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3016 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa53a5f4f5ff4598b6e239d36e9889ef"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\n\n# Set up training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    per_device_train_batch_size=8,    # Adjust based on available RAM\n    per_device_eval_batch_size=8,     # Adjust based on available RAM\n    learning_rate=5e-5,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    fp16=True,  # Enable mixed precision training\n    evaluation_strategy=\"epoch\",  # Evaluate after each epoch\n    save_strategy=\"epoch\",        # Save model after each epoch\n    logging_dir='./logs',         # Directory for storing logs\n    logging_steps=10,\n    save_total_limit=2,           # Limit the total amount of checkpoints\n    load_best_model_at_end=True,  # Load the best model at the end of training\n    report_to=\"none\"              # Disable all reporting integrations\n)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wX8AE8hC5qTy","outputId":"bb890068-0a90-4e74-e6d6-7e5d27e95a93","execution":{"iopub.status.busy":"2024-07-28T11:22:13.272454Z","iopub.execute_input":"2024-07-28T11:22:13.272865Z","iopub.status.idle":"2024-07-28T11:22:13.303022Z","shell.execute_reply.started":"2024-07-28T11:22:13.272836Z","shell.execute_reply":"2024-07-28T11:22:13.302067Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\n# Disable W&B logging\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n","metadata":{"execution":{"iopub.status.busy":"2024-07-28T11:22:15.981052Z","iopub.execute_input":"2024-07-28T11:22:15.981485Z","iopub.status.idle":"2024-07-28T11:22:15.986642Z","shell.execute_reply.started":"2024-07-28T11:22:15.981438Z","shell.execute_reply":"2024-07-28T11:22:15.985481Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,  # If you have an evaluation dataset\n    data_collator=data_collator,\n    tokenizer=tokenizer\n)\n\n# Start training\ntrainer.train()\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":75},"id":"hFlByfYS7Wjj","outputId":"f4d7bbe6-8ed4-4b37-9edd-24c49db7e2c9","execution":{"iopub.status.busy":"2024-07-28T11:22:22.073457Z","iopub.execute_input":"2024-07-28T11:22:22.073877Z","iopub.status.idle":"2024-07-28T11:30:55.826835Z","shell.execute_reply.started":"2024-07-28T11:22:22.073844Z","shell.execute_reply":"2024-07-28T11:30:55.825810Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/data/data_collator.py:656: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/tensor_new.cpp:261.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1131' max='1131' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1131/1131 08:32, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.083300</td>\n      <td>0.080039</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.077700</td>\n      <td>0.079767</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.077700</td>\n      <td>0.079379</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1131, training_loss=0.21880814693308215, metrics={'train_runtime': 513.3162, 'train_samples_per_second': 17.627, 'train_steps_per_second': 2.203, 'total_flos': 1377462748446720.0, 'train_loss': 0.21880814693308215, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"# Save the final model\ntrainer.save_model(\"final_model\")\n\n# Save the tokenizer\ntokenizer.save_pretrained(\"final_tokenizer\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-28T11:31:36.040585Z","iopub.execute_input":"2024-07-28T11:31:36.041051Z","iopub.status.idle":"2024-07-28T11:31:37.169081Z","shell.execute_reply.started":"2024-07-28T11:31:36.041009Z","shell.execute_reply":"2024-07-28T11:31:37.168088Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"('final_tokenizer/tokenizer_config.json',\n 'final_tokenizer/special_tokens_map.json',\n 'final_tokenizer/spiece.model',\n 'final_tokenizer/added_tokens.json',\n 'final_tokenizer/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"# Save the final model\ntrainer.save_model(\"final_model\")\n\n# Save the tokenizer\ntokenizer.save_pretrained(\"final_tokenizer\")\n","metadata":{"id":"_chT4BuP7dSN","execution":{"iopub.status.busy":"2024-07-28T11:33:16.704840Z","iopub.execute_input":"2024-07-28T11:33:16.705669Z","iopub.status.idle":"2024-07-28T11:33:18.387855Z","shell.execute_reply.started":"2024-07-28T11:33:16.705635Z","shell.execute_reply":"2024-07-28T11:33:18.386713Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"('final_tokenizer/tokenizer_config.json',\n 'final_tokenizer/special_tokens_map.json',\n 'final_tokenizer/spiece.model',\n 'final_tokenizer/added_tokens.json',\n 'final_tokenizer/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"import shutil\n\n# Zip the final_model directory\nshutil.make_archive('final_model', 'zip', 'final_model')\n\n# Zip the final_tokenizer directory\nshutil.make_archive('final_tokenizer', 'zip', 'final_tokenizer')\n","metadata":{"execution":{"iopub.status.busy":"2024-07-28T11:33:31.569823Z","iopub.execute_input":"2024-07-28T11:33:31.570201Z","iopub.status.idle":"2024-07-28T11:34:24.306998Z","shell.execute_reply.started":"2024-07-28T11:33:31.570172Z","shell.execute_reply":"2024-07-28T11:34:24.306000Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/final_tokenizer.zip'"},"metadata":{}}]},{"cell_type":"code","source":"from IPython.display import FileLink\n\n# Create download links for the zipped files\nFileLink(r'final_model.zip')\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-28T11:34:46.263790Z","iopub.execute_input":"2024-07-28T11:34:46.264515Z","iopub.status.idle":"2024-07-28T11:34:46.270784Z","shell.execute_reply.started":"2024-07-28T11:34:46.264477Z","shell.execute_reply":"2024-07-28T11:34:46.269847Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/final_model.zip","text/html":"<a href='final_model.zip' target='_blank'>final_model.zip</a><br>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Testing the model ","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n# Check if GPU is available and set device accordingly\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"final_tokenizer\")\n\n# Load the model and move it to the correct device\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"final_model\").to(device)\n\n# Function to correct text\ndef correct_text(input_text):\n    # Tokenize the input text\n    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=128, truncation=True, padding=\"max_length\")\n\n    # Move input tensors to the correct device\n    inputs = {key: value.to(device) for key, value in inputs.items()}\n\n    # Generate prediction\n    with torch.no_grad():\n        outputs = model.generate(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_length=128)\n\n    # Decode the output\n    corrected_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return corrected_text\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-28T11:41:21.061721Z","iopub.execute_input":"2024-07-28T11:41:21.062437Z","iopub.status.idle":"2024-07-28T11:41:22.331002Z","shell.execute_reply.started":"2024-07-28T11:41:21.062385Z","shell.execute_reply":"2024-07-28T11:41:22.329938Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# Example input\ninput_text = \"she are going to school by bus.\"\n\n# Get the corrected output\ncorrected_text = correct_text(input_text)\n\nprint(\"Original:\", input_text)\nprint(\"Corrected:\", corrected_text)","metadata":{"execution":{"iopub.status.busy":"2024-07-28T11:41:27.622266Z","iopub.execute_input":"2024-07-28T11:41:27.622895Z","iopub.status.idle":"2024-07-28T11:41:27.793931Z","shell.execute_reply.started":"2024-07-28T11:41:27.622861Z","shell.execute_reply":"2024-07-28T11:41:27.792954Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Original: she are going to school by bus.\nCorrected: she is going to school by bus.\n","output_type":"stream"}]}]}